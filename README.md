
# MNIST Classification with PyTorch  

## Objective  
This lab focuses on implementing and comparing different neural architectures for classifying the MNIST dataset using PyTorch.  

## Work Done  

### Part 1: CNN & Faster R-CNN  
1. **CNN Classifier**: Built a CNN model with Conv, Pooling, and FC layers, optimized with hyperparameters (kernel size, stride, padding, etc.).  
2. **Faster R-CNN**: Adapted Faster R-CNN for MNIST classification.  
3. **Comparison**: Evaluated both models using accuracy, F1 score, loss, and training time.  
4. **Transfer Learning**: Fine-tuned pretrained models (VGG16, AlexNet) and compared results.  

### Part 2: Vision Transformer (ViT)  
1. **ViT from Scratch**: Implemented a Vision Transformer based on a tutorial and trained it on MNIST.  
2. **Comparison**: Compared ViT results with CNN and Faster R-CNN.  

## Key Takeaways  
- Gained hands-on experience with PyTorch for building CNNs, R-CNNs, and ViTs.  
- Learned the impact of hyperparameters and architecture choices on model performance.  
- Understood the advantages of transfer learning and transformer-based models in vision tasks.  


